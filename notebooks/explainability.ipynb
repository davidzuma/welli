{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ea1acc",
   "metadata": {},
   "source": [
    "# Model Explainability with LIME\n",
    "\n",
    "This notebook provides explainability for the Welli ML models using LIME (Local Interpretable Model-agnostic Explanations).\n",
    "\n",
    "**What you'll find here:**\n",
    "- **Individual prediction explanations** for both clustering and churn models\n",
    "- **Feature importance analysis** at the instance level\n",
    "- **Model behavior understanding** for different user types\n",
    "- **Trust and transparency** in model decisions\n",
    "\n",
    "**Requirements:**\n",
    "- Trained models from `user_clustering.ipynb` and `churn_model.ipynb`\n",
    "- Training dataset from `data_creation.ipynb`\n",
    "\n",
    "LIME helps answer: *\"Why did the model make this specific prediction for this user?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1acba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing LIME...\n",
      "Collecting lime\n",
      "  Using cached lime-0.2.0.1.tar.gz (275 kB)\n",
      "Collecting lime\n",
      "  Using cached lime-0.2.0.1.tar.gz (275 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l  Preparing metadata (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (3.7.2)\n",
      "Requirement already satisfied: numpy in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.26.1)\n",
      "Requirement already satisfied: scipy in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.11.2)\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (3.7.2)\n",
      "Requirement already satisfied: numpy in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.26.1)\n",
      "Requirement already satisfied: scipy in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.11.2)\n",
      "Collecting tqdm (from lime)\n",
      "Collecting tqdm (from lime)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.3.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from lime) (1.3.0)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-macosx_12_0_arm64.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/13.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m  Downloading scikit_image-0.25.2-cp310-cp310-macosx_12_0_arm64.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from lime)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/22.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mCollecting scipy (from lime)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=3.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=3.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "Collecting pillow>=10.1 (from scikit-image>=0.12->lime)\n",
      "Collecting pillow>=10.1 (from scikit-image>=0.12->lime)\n",
      "  Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading pillow-11.3.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio!=2.35.0,>=2.33 (from scikit-image>=0.12->lime)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image>=0.12->lime)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "\u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mCollecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.5/226.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (23.2)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.5/226.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-image>=0.12->lime) (23.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.12->lime)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.12->lime)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25lRequirement already satisfied: six>=1.5 in /Users/davidzumaquero/miniconda3/envs/opendata_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283839 sha256=8e87d1183eb1b6fb72b9304c7b0088da9f5739d51aec834e822b8a1e962b1870\n",
      "  Stored in directory: /Users/davidzumaquero/Library/Caches/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
      "Successfully built lime\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283839 sha256=8e87d1183eb1b6fb72b9304c7b0088da9f5739d51aec834e822b8a1e962b1870\n",
      "  Stored in directory: /Users/davidzumaquero/Library/Caches/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
      "Successfully built lime\n",
      "Installing collected packages: tqdm, tifffile, scipy, pillow, lazy-loader, imageio, scikit-image, lime\n",
      "Installing collected packages: tqdm, tifffile, scipy, pillow, lazy-loader, imageio, scikit-image, lime\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.2\n",
      "    Uninstalling scipy-1.11.2:\n",
      "    Uninstalling scipy-1.11.2:\n",
      "      Successfully uninstalled scipy-1.11.2\n",
      "      Successfully uninstalled scipy-1.11.2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.26.0 requires pillow<10,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed imageio-2.37.0 lazy-loader-0.4 lime-0.2.0.1 pillow-9.5.0 scikit-image-0.25.2 scipy-1.15.3 tifffile-2025.5.10 tqdm-4.67.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.26.0 requires pillow<10,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed imageio-2.37.0 lazy-loader-0.4 lime-0.2.0.1 pillow-9.5.0 scikit-image-0.25.2 scipy-1.15.3 tifffile-2025.5.10 tqdm-4.67.1\n",
      "✅ LIME installed successfully\n",
      "✅ LIME installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install LIME if not already installed\n",
    "try:\n",
    "    import lime\n",
    "    print(\"✅ LIME already installed\")\n",
    "except ImportError:\n",
    "    print(\"📦 Installing LIME...\")\n",
    "    !pip install lime\n",
    "    import lime\n",
    "    print(\"✅ LIME installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465dd8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Ready to explain model predictions...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from lime import lime_tabular\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to explain model predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset and models\n",
    "data_path = \"../data/training_dataset.csv\"\n",
    "models_dir = {\n",
    "    \"clustering\": \"../ml_models/clustering/\",\n",
    "    \"churn\": \"../ml_models/churn_classification/\"\n",
    "}\n",
    "\n",
    "# Check if required files exist\n",
    "required_files = [\n",
    "    \"training_dataset.csv\",\n",
    "    \"kmeans_model.joblib\", \n",
    "    \"clustering_scaler.joblib\",\n",
    "    \"cluster_info.json\",\n",
    "    \"churn_model.joblib\",\n",
    "    \"churn_scaler.joblib\"\n",
    "]\n",
    "\n",
    "# Load models\n",
    "kmeans_model = joblib.load(os.path.join(models_dir[\"clustering\"], \"kmeans_model.joblib\"))\n",
    "clustering_scaler = joblib.load(os.path.join(models_dir[\"clustering\"], \"clustering_scaler.joblib\"))\n",
    "churn_model = joblib.load(os.path.join(models_dir[\"churn\"], \"churn_model.joblib\"))\n",
    "churn_scaler = joblib.load(os.path.join(models_dir[\"churn\"], \"churn_scaler.joblib\"))\n",
    "\n",
    "with open(os.path.join(models_dir[\"clustering\"], \"cluster_info.json\"), 'r') as f:\n",
    "    cluster_info = json.load(f)\n",
    "    # Load the training dataset\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"📂 Training dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"🤖 All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57679e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Feature sets defined:\n",
      "   • Clustering features: 6\n",
      "   • Churn features: 8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   • Churn features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(churn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Prepare feature matrices\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m X_clustering \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[clustering_features]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     28\u001b[0m X_churn \u001b[38;5;241m=\u001b[39m df[churn_features]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📐 Feature matrices prepared:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Define feature sets (exactly as used in the models)\n",
    "clustering_features = [\n",
    "    'session_count',\n",
    "    'avg_session_duration', \n",
    "    'streak_length',\n",
    "    'preferred_time_of_day',\n",
    "    'content_engagement_rate',\n",
    "    'notification_response_rate'\n",
    "]\n",
    "\n",
    "churn_features = [\n",
    "    \"days_since_signup\",\n",
    "    \"total_sessions\", \n",
    "    \"avg_session_duration\",\n",
    "    \"streak_length\",\n",
    "    \"last_login_days_ago\",\n",
    "    \"content_completion_rate\",\n",
    "    \"notification_response_rate\",\n",
    "    \"goal_progress_percentage\"\n",
    "]\n",
    "\n",
    "print(\"📋 Feature sets defined:\")\n",
    "print(f\"   • Clustering features: {len(clustering_features)}\")\n",
    "print(f\"   • Churn features: {len(churn_features)}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_clustering = df[clustering_features].values\n",
    "X_churn = df[churn_features].values\n",
    "\n",
    "print(f\"\\n📐 Feature matrices prepared:\")\n",
    "print(f\"   • Clustering: {X_clustering.shape}\")\n",
    "print(f\"   • Churn: {X_churn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wrapper functions for LIME (LIME expects probability outputs)\n",
    "def clustering_predict_proba(X):\n",
    "    \"\"\"\n",
    "    Wrapper for clustering model to return probabilities for LIME\n",
    "    Since KMeans doesn't have predict_proba, we'll use distances to clusters\n",
    "    \"\"\"\n",
    "    X_scaled = clustering_scaler.transform(X)\n",
    "    distances = kmeans_model.transform(X_scaled)\n",
    "    \n",
    "    # Convert distances to probabilities (closer = higher probability)\n",
    "    # Use negative distances and apply softmax\n",
    "    neg_distances = -distances\n",
    "    exp_distances = np.exp(neg_distances - np.max(neg_distances, axis=1, keepdims=True))\n",
    "    probabilities = exp_distances / np.sum(exp_distances, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def churn_predict_proba(X):\n",
    "    \"\"\"\n",
    "    Wrapper for churn model to return probabilities for LIME\n",
    "    \"\"\"\n",
    "    X_scaled = churn_scaler.transform(X)\n",
    "    return churn_model.predict_proba(X_scaled)\n",
    "\n",
    "print(\"🔧 Model wrapper functions created for LIME compatibility\")\n",
    "\n",
    "# Test the wrapper functions\n",
    "test_sample_clustering = X_clustering[:1]\n",
    "test_sample_churn = X_churn[:1]\n",
    "\n",
    "clustering_probs = clustering_predict_proba(test_sample_clustering)\n",
    "churn_probs = churn_predict_proba(test_sample_churn)\n",
    "\n",
    "print(f\"\\n✅ Wrapper functions tested:\")\n",
    "print(f\"   • Clustering probabilities shape: {clustering_probs.shape}\")\n",
    "print(f\"   • Churn probabilities shape: {churn_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cde4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LIME explainers\n",
    "print(\"🔍 Creating LIME explainers...\")\n",
    "\n",
    "# Clustering explainer\n",
    "clustering_explainer = LimeTabularExplainer(\n",
    "    X_clustering,\n",
    "    feature_names=clustering_features,\n",
    "    class_names=[f\"Cluster {i}: {cluster_info['clusters'][str(i)]['name']}\" \n",
    "                 for i in range(len(cluster_info['clusters']))],\n",
    "    mode='classification',\n",
    "    discretize_continuous=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Churn explainer\n",
    "churn_explainer = LimeTabularExplainer(\n",
    "    X_churn,\n",
    "    feature_names=churn_features,\n",
    "    class_names=['Active User', 'Churned User'],\n",
    "    mode='classification',\n",
    "    discretize_continuous=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ LIME explainers created successfully!\")\n",
    "print(f\"   • Clustering explainer: {len(cluster_info['clusters'])} clusters\")\n",
    "print(f\"   • Churn explainer: 2 classes (Active/Churned)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b644a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample users for explanation\n",
    "print(\"👥 Selecting sample users for explanation...\")\n",
    "\n",
    "# Select diverse users for explanation\n",
    "sample_indices = []\n",
    "\n",
    "# One user from each type\n",
    "for user_type in df['user_type'].unique():\n",
    "    user_subset = df[df['user_type'] == user_type]\n",
    "    sample_indices.append(user_subset.index[0])  # First user of each type\n",
    "\n",
    "# Add some random users\n",
    "np.random.seed(42)\n",
    "additional_samples = np.random.choice(df.index, size=3, replace=False)\n",
    "sample_indices.extend(additional_samples)\n",
    "\n",
    "# Remove duplicates\n",
    "sample_indices = list(set(sample_indices))\n",
    "\n",
    "print(f\"📝 Selected {len(sample_indices)} users for explanation:\")\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    user_type = df.loc[idx, 'user_type']\n",
    "    churn_status = 'Churned' if df.loc[idx, 'churn'] == 1 else 'Active'\n",
    "    print(f\"   {i+1}. User {idx}: {user_type} user, {churn_status}\")\n",
    "\n",
    "# Display sample user details\n",
    "sample_users = df.loc[sample_indices]\n",
    "print(\"\\n📊 Sample user characteristics:\")\n",
    "display(sample_users[['user_type', 'churn'] + clustering_features + churn_features].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explain clustering predictions\n",
    "def explain_clustering_prediction(user_index, show_plot=True):\n",
    "    \"\"\"\n",
    "    Explain why a user was assigned to a specific cluster\n",
    "    \"\"\"\n",
    "    user_data = X_clustering[user_index]\n",
    "    user_info = df.loc[user_index]\n",
    "    \n",
    "    # Get actual prediction\n",
    "    probabilities = clustering_predict_proba(user_data.reshape(1, -1))[0]\n",
    "    predicted_cluster = np.argmax(probabilities)\n",
    "    cluster_name = cluster_info['clusters'][str(predicted_cluster)]['name']\n",
    "    \n",
    "    print(f\"\\n🎯 CLUSTERING EXPLANATION - User {user_index}\")\n",
    "    print(f\"User Type: {user_info['user_type']}\")\n",
    "    print(f\"Predicted Cluster: {predicted_cluster} - {cluster_name}\")\n",
    "    print(f\"Confidence: {probabilities[predicted_cluster]:.3f}\")\n",
    "    \n",
    "    # Generate LIME explanation\n",
    "    explanation = clustering_explainer.explain_instance(\n",
    "        user_data, \n",
    "        clustering_predict_proba,\n",
    "        num_features=len(clustering_features),\n",
    "        top_labels=len(cluster_info['clusters'])\n",
    "    )\n",
    "    \n",
    "    if show_plot:\n",
    "        # Show explanation plot\n",
    "        fig = explanation.as_pyplot_figure(label=predicted_cluster)\n",
    "        plt.title(f'Clustering Explanation - User {user_index}\\n'\n",
    "                 f'Predicted: Cluster {predicted_cluster} - {cluster_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Return explanation data\n",
    "    return explanation, predicted_cluster, probabilities\n",
    "\n",
    "# Function to explain churn predictions\n",
    "def explain_churn_prediction(user_index, show_plot=True):\n",
    "    \"\"\"\n",
    "    Explain why a user was predicted to churn or stay\n",
    "    \"\"\"\n",
    "    user_data = X_churn[user_index]\n",
    "    user_info = df.loc[user_index]\n",
    "    \n",
    "    # Get actual prediction\n",
    "    probabilities = churn_predict_proba(user_data.reshape(1, -1))[0]\n",
    "    churn_probability = probabilities[1]\n",
    "    predicted_churn = int(churn_probability >= 0.5)\n",
    "    actual_churn = user_info['churn']\n",
    "    \n",
    "    # Determine risk level\n",
    "    if churn_probability >= 0.7:\n",
    "        risk_level = \"High\"\n",
    "    elif churn_probability >= 0.4:\n",
    "        risk_level = \"Medium\"\n",
    "    else:\n",
    "        risk_level = \"Low\"\n",
    "    \n",
    "    print(f\"\\n🎯 CHURN EXPLANATION - User {user_index}\")\n",
    "    print(f\"User Type: {user_info['user_type']}\")\n",
    "    print(f\"Actual Status: {'Churned' if actual_churn else 'Active'}\")\n",
    "    print(f\"Predicted Status: {'Churned' if predicted_churn else 'Active'}\")\n",
    "    print(f\"Churn Probability: {churn_probability:.3f}\")\n",
    "    print(f\"Risk Level: {risk_level}\")\n",
    "    \n",
    "    # Generate LIME explanation\n",
    "    explanation = churn_explainer.explain_instance(\n",
    "        user_data, \n",
    "        churn_predict_proba,\n",
    "        num_features=len(churn_features)\n",
    "    )\n",
    "    \n",
    "    if show_plot:\n",
    "        # Show explanation plot\n",
    "        fig = explanation.as_pyplot_figure(label=1)  # Explain churn class\n",
    "        plt.title(f'Churn Explanation - User {user_index}\\n'\n",
    "                 f'Churn Probability: {churn_probability:.3f} ({risk_level} Risk)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Return explanation data\n",
    "    return explanation, churn_probability, probabilities\n",
    "\n",
    "print(\"🔧 Explanation functions defined and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f06ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain clustering for sample users\n",
    "print(\"🔍 CLUSTERING EXPLANATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "clustering_explanations = {}\n",
    "\n",
    "for i, user_idx in enumerate(sample_indices[:3]):  # Show first 3 users\n",
    "    explanation, predicted_cluster, probabilities = explain_clustering_prediction(user_idx)\n",
    "    clustering_explanations[user_idx] = {\n",
    "        'explanation': explanation,\n",
    "        'predicted_cluster': predicted_cluster,\n",
    "        'probabilities': probabilities\n",
    "    }\n",
    "    \n",
    "    if i < len(sample_indices) - 1:\n",
    "        print(\"\\n\" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain churn predictions for sample users\n",
    "print(\"🚨 CHURN PREDICTION EXPLANATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "churn_explanations = {}\n",
    "\n",
    "for i, user_idx in enumerate(sample_indices[:3]):  # Show first 3 users\n",
    "    explanation, churn_prob, probabilities = explain_churn_prediction(user_idx)\n",
    "    churn_explanations[user_idx] = {\n",
    "        'explanation': explanation,\n",
    "        'churn_probability': churn_prob,\n",
    "        'probabilities': probabilities\n",
    "    }\n",
    "    \n",
    "    if i < len(sample_indices) - 1:\n",
    "        print(\"\\n\" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b211ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance across explanations\n",
    "def analyze_feature_importance_patterns():\n",
    "    \"\"\"\n",
    "    Analyze common patterns in feature importance across different explanations\n",
    "    \"\"\"\n",
    "    print(\"📈 FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze clustering feature importance\n",
    "    clustering_feature_importance = {feature: [] for feature in clustering_features}\n",
    "    \n",
    "    for user_idx in sample_indices:\n",
    "        explanation, _, _ = explain_clustering_prediction(user_idx, show_plot=False)\n",
    "        \n",
    "        # Get feature importance for the predicted class\n",
    "        predicted_cluster = clustering_explanations.get(user_idx, {}).get('predicted_cluster', 0)\n",
    "        feature_importance = explanation.as_list(label=predicted_cluster)\n",
    "        \n",
    "        for feature, importance in feature_importance:\n",
    "            # Clean feature name (remove value information)\n",
    "            clean_feature = feature.split(' ')[0] if ' ' in feature else feature\n",
    "            if clean_feature in clustering_feature_importance:\n",
    "                clustering_feature_importance[clean_feature].append(abs(importance))\n",
    "    \n",
    "    # Calculate average importance\n",
    "    clustering_avg_importance = {}\n",
    "    for feature, importances in clustering_feature_importance.items():\n",
    "        if importances:\n",
    "            clustering_avg_importance[feature] = np.mean(importances)\n",
    "    \n",
    "    # Analyze churn feature importance  \n",
    "    churn_feature_importance = {feature: [] for feature in churn_features}\n",
    "    \n",
    "    for user_idx in sample_indices:\n",
    "        explanation, _, _ = explain_churn_prediction(user_idx, show_plot=False)\n",
    "        \n",
    "        # Get feature importance for churn class (label=1)\n",
    "        feature_importance = explanation.as_list(label=1)\n",
    "        \n",
    "        for feature, importance in feature_importance:\n",
    "            # Clean feature name\n",
    "            clean_feature = feature.split(' ')[0] if ' ' in feature else feature\n",
    "            if clean_feature in churn_feature_importance:\n",
    "                churn_feature_importance[clean_feature].append(abs(importance))\n",
    "    \n",
    "    # Calculate average importance\n",
    "    churn_avg_importance = {}\n",
    "    for feature, importances in churn_feature_importance.items():\n",
    "        if importances:\n",
    "            churn_avg_importance[feature] = np.mean(importances)\n",
    "    \n",
    "    return clustering_avg_importance, churn_avg_importance\n",
    "\n",
    "# Run the analysis\n",
    "clustering_importance, churn_importance = analyze_feature_importance_patterns()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n🏷️  Most Important Features for Clustering:\")\n",
    "sorted_clustering = sorted(clustering_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, importance) in enumerate(sorted_clustering[:5], 1):\n",
    "    print(f\"   {i}. {feature}: {importance:.3f}\")\n",
    "\n",
    "print(\"\\n🚨 Most Important Features for Churn Prediction:\")\n",
    "sorted_churn = sorted(churn_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, importance) in enumerate(sorted_churn[:5], 1):\n",
    "    print(f\"   {i}. {feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive explanation dashboard\n",
    "def create_explanation_dashboard(user_index):\n",
    "    \"\"\"\n",
    "    Create a comprehensive dashboard explaining both clustering and churn predictions\n",
    "    \"\"\"\n",
    "    user_info = df.loc[user_index]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Model Explanations Dashboard - User {user_index}\\n'\n",
    "                f'Type: {user_info[\"user_type\"].title()}, '\n",
    "                f'Status: {\"Churned\" if user_info[\"churn\"] else \"Active\"}', \n",
    "                fontsize=16)\n",
    "    \n",
    "    # 1. User feature values (radar chart style)\n",
    "    user_clustering_data = X_clustering[user_index]\n",
    "    user_churn_data = X_churn[user_index]\n",
    "    \n",
    "    # Normalize features for radar chart\n",
    "    clustering_normalized = (user_clustering_data - X_clustering.min(axis=0)) / (X_clustering.max(axis=0) - X_clustering.min(axis=0))\n",
    "    \n",
    "    axes[0,0].bar(range(len(clustering_features)), clustering_normalized, alpha=0.7)\n",
    "    axes[0,0].set_title('Clustering Features (Normalized)')\n",
    "    axes[0,0].set_xticks(range(len(clustering_features)))\n",
    "    axes[0,0].set_xticklabels([f.replace('_', '\\n') for f in clustering_features], rotation=45, ha='right')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Clustering explanation\n",
    "    clustering_exp, pred_cluster, cluster_probs = explain_clustering_prediction(user_index, show_plot=False)\n",
    "    cluster_names = [cluster_info['clusters'][str(i)]['name'] for i in range(len(cluster_info['clusters']))]\n",
    "    \n",
    "    axes[0,1].bar(range(len(cluster_probs)), cluster_probs, alpha=0.7)\n",
    "    axes[0,1].set_title(f'Cluster Probabilities\\nPredicted: {cluster_names[pred_cluster]}')\n",
    "    axes[0,1].set_xticks(range(len(cluster_probs)))\n",
    "    axes[0,1].set_xticklabels([f'C{i}\\n{name[:8]}...' if len(name) > 8 else f'C{i}\\n{name}' \n",
    "                              for i, name in enumerate(cluster_names)], rotation=45, ha='right')\n",
    "    axes[0,1].set_ylabel('Probability')\n",
    "    \n",
    "    # Highlight predicted cluster\n",
    "    axes[0,1].patches[pred_cluster].set_color('red')\n",
    "    axes[0,1].patches[pred_cluster].set_alpha(0.9)\n",
    "    \n",
    "    # 3. Churn features\n",
    "    churn_normalized = (user_churn_data - X_churn.min(axis=0)) / (X_churn.max(axis=0) - X_churn.min(axis=0))\n",
    "    \n",
    "    axes[1,0].bar(range(len(churn_features)), churn_normalized, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('Churn Features (Normalized)')\n",
    "    axes[1,0].set_xticks(range(len(churn_features)))\n",
    "    axes[1,0].set_xticklabels([f.replace('_', '\\n') for f in churn_features], rotation=45, ha='right')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Churn explanation\n",
    "    churn_exp, churn_prob, churn_probs = explain_churn_prediction(user_index, show_plot=False)\n",
    "    \n",
    "    risk_level = \"High\" if churn_prob >= 0.7 else \"Medium\" if churn_prob >= 0.4 else \"Low\"\n",
    "    risk_color = \"red\" if risk_level == \"High\" else \"orange\" if risk_level == \"Medium\" else \"green\"\n",
    "    \n",
    "    axes[1,1].bar(['Active', 'Churned'], churn_probs, alpha=0.7, color=['green', 'red'])\n",
    "    axes[1,1].set_title(f'Churn Probability: {churn_prob:.3f}\\nRisk Level: {risk_level}')\n",
    "    axes[1,1].set_ylabel('Probability')\n",
    "    \n",
    "    # Add risk level indicator\n",
    "    axes[1,1].axhline(y=churn_prob, color=risk_color, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed explanation\n",
    "    print(f\"\\n📊 DETAILED EXPLANATION FOR USER {user_index}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"User Profile: {user_info['user_type'].title()} user, {('Churned' if user_info['churn'] else 'Active')}\")\n",
    "    \n",
    "    print(f\"\\n🏷️  Clustering Prediction:\")\n",
    "    print(f\"   Assigned to: Cluster {pred_cluster} - {cluster_names[pred_cluster]}\")\n",
    "    print(f\"   Confidence: {cluster_probs[pred_cluster]:.3f}\")\n",
    "    print(f\"   Description: {cluster_info['clusters'][str(pred_cluster)]['description']}\")\n",
    "    \n",
    "    print(f\"\\n🚨 Churn Prediction:\")\n",
    "    print(f\"   Churn Probability: {churn_prob:.3f}\")\n",
    "    print(f\"   Risk Level: {risk_level}\")\n",
    "    print(f\"   Recommendation: {'Immediate intervention needed' if risk_level == 'High' else 'Monitor closely' if risk_level == 'Medium' else 'Continue current engagement'}\")\n",
    "\n",
    "# Create dashboard for a sample user\n",
    "print(\"🎛️  Creating comprehensive explanation dashboard...\")\n",
    "create_explanation_dashboard(sample_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive explanation function\n",
    "def interactive_user_explanation(user_id=None):\n",
    "    \"\"\"\n",
    "    Interactive function to explain any user's predictions\n",
    "    \"\"\"\n",
    "    if user_id is None:\n",
    "        # Show available user types and sample indices\n",
    "        print(\"🔍 Available Users for Explanation:\")\n",
    "        print(\"\\nUser Types in Dataset:\")\n",
    "        for user_type in df['user_type'].unique():\n",
    "            count = len(df[df['user_type'] == user_type])\n",
    "            churn_rate = df[df['user_type'] == user_type]['churn'].mean()\n",
    "            print(f\"   • {user_type.title()}: {count:,} users (churn rate: {churn_rate:.1%})\")\n",
    "        \n",
    "        print(f\"\\n📋 Sample User Indices: {sample_indices[:10]}\")\n",
    "        print(\"\\nTo explain a specific user, call: interactive_user_explanation(user_index)\")\n",
    "        print(\"Example: interactive_user_explanation(42)\")\n",
    "        return\n",
    "    \n",
    "    if user_id not in df.index:\n",
    "        print(f\"❌ User {user_id} not found in dataset!\")\n",
    "        print(f\"Available range: 0 to {len(df)-1}\")\n",
    "        return\n",
    "    \n",
    "    # Create full explanation for the user\n",
    "    create_explanation_dashboard(user_id)\n",
    "    \n",
    "    # Show specific feature contributions\n",
    "    print(f\"\\n🔍 TOP FEATURE CONTRIBUTIONS:\")\n",
    "    \n",
    "    # Clustering features\n",
    "    clustering_exp, _, _ = explain_clustering_prediction(user_id, show_plot=False)\n",
    "    pred_cluster = clustering_explanations.get(user_id, {}).get('predicted_cluster', 0)\n",
    "    clustering_features_list = clustering_exp.as_list(label=pred_cluster)\n",
    "    \n",
    "    print(f\"\\nClustering (top 3 features):\")\n",
    "    for i, (feature, importance) in enumerate(clustering_features_list[:3], 1):\n",
    "        direction = \"increases\" if importance > 0 else \"decreases\"\n",
    "        print(f\"   {i}. {feature} {direction} cluster probability by {abs(importance):.3f}\")\n",
    "    \n",
    "    # Churn features\n",
    "    churn_exp, _, _ = explain_churn_prediction(user_id, show_plot=False)\n",
    "    churn_features_list = churn_exp.as_list(label=1)\n",
    "    \n",
    "    print(f\"\\nChurn Prediction (top 3 features):\")\n",
    "    for i, (feature, importance) in enumerate(churn_features_list[:3], 1):\n",
    "        direction = \"increases\" if importance > 0 else \"decreases\"\n",
    "        print(f\"   {i}. {feature} {direction} churn probability by {abs(importance):.3f}\")\n",
    "\n",
    "# Show usage instructions\n",
    "interactive_user_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d05bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanations for different user types\n",
    "print(\"👥 EXPLANATIONS BY USER TYPE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "user_type_examples = {}\n",
    "\n",
    "for user_type in df['user_type'].unique():\n",
    "    # Find a representative user of this type\n",
    "    user_subset = df[df['user_type'] == user_type]\n",
    "    \n",
    "    # Pick a user close to the median for this type\n",
    "    median_engagement = user_subset['content_engagement_rate'].median()\n",
    "    closest_user = user_subset.iloc[(user_subset['content_engagement_rate'] - median_engagement).abs().argsort()[:1]]\n",
    "    user_idx = closest_user.index[0]\n",
    "    \n",
    "    user_type_examples[user_type] = user_idx\n",
    "    \n",
    "    print(f\"\\n🎯 Representative {user_type.title()} User (Index: {user_idx}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Quick explanation\n",
    "    clustering_exp, pred_cluster, cluster_probs = explain_clustering_prediction(user_idx, show_plot=False)\n",
    "    churn_exp, churn_prob, _ = explain_churn_prediction(user_idx, show_plot=False)\n",
    "    \n",
    "    cluster_name = cluster_info['clusters'][str(pred_cluster)]['name']\n",
    "    risk_level = \"High\" if churn_prob >= 0.7 else \"Medium\" if churn_prob >= 0.4 else \"Low\"\n",
    "    \n",
    "    print(f\"Cluster Assignment: {cluster_name} (confidence: {cluster_probs[pred_cluster]:.3f})\")\n",
    "    print(f\"Churn Risk: {risk_level} (probability: {churn_prob:.3f})\")\n",
    "    \n",
    "    # Top contributing features\n",
    "    clustering_top = clustering_exp.as_list(label=pred_cluster)[0]\n",
    "    churn_top = churn_exp.as_list(label=1)[0]\n",
    "    \n",
    "    print(f\"Key clustering factor: {clustering_top[0]} ({clustering_top[1]:+.3f})\")\n",
    "    print(f\"Key churn factor: {churn_top[0]} ({churn_top[1]:+.3f})\")\n",
    "\n",
    "print(f\"\\n✅ Generated explanations for {len(user_type_examples)} user types\")\n",
    "print(\"\\nTo see detailed explanations for any of these users, use:\")\n",
    "for user_type, user_idx in user_type_examples.items():\n",
    "    print(f\"   interactive_user_explanation({user_idx})  # {user_type} user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5de13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and insights\n",
    "print(\"📋 MODEL EXPLAINABILITY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS FROM LIME EXPLANATIONS:\")\n",
    "\n",
    "print(\"\\n1. 🏷️  Clustering Model:\")\n",
    "print(\"   • Groups users based on engagement patterns and session behavior\")\n",
    "print(\"   • Most important features typically include session count and engagement rate\")\n",
    "print(\"   • Time preferences and notification responses help distinguish user types\")\n",
    "\n",
    "print(\"\\n2. 🚨 Churn Model:\")\n",
    "print(\"   • Focuses on recency and consistency metrics\")\n",
    "print(\"   • Days since last login and goal progress are strong predictors\")\n",
    "print(\"   • Content completion rate indicates user satisfaction\")\n",
    "\n",
    "print(\"\\n3. 🔍 Model Transparency:\")\n",
    "print(\"   • LIME provides instance-level explanations for individual users\")\n",
    "print(\"   • Feature contributions can be positive (increase probability) or negative\")\n",
    "print(\"   • Explanations help validate model behavior and build trust\")\n",
    "\n",
    "print(\"\\n📊 PRACTICAL APPLICATIONS:\")\n",
    "print(\"\\n• 🎯 Personalized Interventions:\")\n",
    "print(\"     - Target specific features that drive churn for each user\")\n",
    "print(\"     - Customize content based on cluster characteristics\")\n",
    "\n",
    "print(\"\\n• 📈 Product Insights:\")\n",
    "print(\"     - Understand which features matter most for different user types\")\n",
    "print(\"     - Identify opportunities for app improvements\")\n",
    "\n",
    "print(\"\\n• 🔧 Model Debugging:\")\n",
    "print(\"     - Detect biased or unexpected model behavior\")\n",
    "print(\"     - Validate model decisions against domain expertise\")\n",
    "\n",
    "print(\"\\n✨ NEXT STEPS:\")\n",
    "print(\"\\n1. Use interactive_user_explanation(user_id) to explore specific users\")\n",
    "print(\"2. Monitor feature importance patterns over time\")\n",
    "print(\"3. Integrate explanations into the production application\")\n",
    "print(\"4. Use insights to improve model features and business logic\")\n",
    "\n",
    "print(\"\\n🎉 Model explainability analysis complete!\")\n",
    "print(\"Your models are now transparent and interpretable! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
